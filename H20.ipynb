{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61494020",
   "metadata": {},
   "source": [
    "# H2O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56d4d6f",
   "metadata": {},
   "source": [
    "## Headers Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3807ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fcd3a5",
   "metadata": {},
   "source": [
    "## Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f6da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"gpt2-large\"\n",
    "generation_length = 512  # The number of tokens to generate for the test\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Loading model: {model_name} (with attentions)...\")\n",
    "\n",
    "# --- Load Model and Tokenizer ---\n",
    "# CRITICAL: We need 'output_attentions=True' to see which tokens are important.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True).to(device)\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0363cd82",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc660e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_inference_h2o(strategy, window_size=None, heavy_hitters_to_keep=4):\n",
    "    \"\"\"\n",
    "    Generates tokens and measures latency for baseline and a simplified H2O strategy.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running test for strategy: '{strategy}' ---\")\n",
    "    \n",
    "    input_ids = torch.tensor([[tokenizer.bos_token_id]], dtype=torch.long).to(device)\n",
    "    past_key_values = None\n",
    "    timings = []\n",
    "    \n",
    "    # We'll use this to track the \"importance\" of each token in the cache\n",
    "    attention_scores = torch.zeros(generation_length + 1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(generation_length):\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            # Model forward pass\n",
    "            outputs = model(input_ids=input_ids[:, -1:], past_key_values=past_key_values, use_cache=True)\n",
    "            \n",
    "            # --- Logic for H2O: Track Attention Scores ---\n",
    "            if strategy == 'h2o' and outputs.attentions is not None:\n",
    "                # Go through each attention layer\n",
    "                for layer_attention in outputs.attentions:\n",
    "                    # Sum the attention scores from the current token to all past tokens\n",
    "                    # This gives us a rough measure of how much the model \"looked back\" at each past token\n",
    "                    current_attention_to_past = layer_attention[0, :, -1, :-1].sum(dim=0)\n",
    "                    if current_attention_to_past.numel() > 0:\n",
    "                        attention_scores[:i] += current_attention_to_past\n",
    "\n",
    "            next_token_id = torch.argmax(outputs.logits[:, -1, :], dim=-1).unsqueeze(-1)\n",
    "            input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "            # --- KV Cache Eviction Logic ---\n",
    "            if past_key_values is None:\n",
    "                end_time = time.perf_counter()\n",
    "                timings.append((end_time - start_time) * 1000)\n",
    "                continue\n",
    "\n",
    "            current_cache_size = past_key_values[0][0].shape[2]\n",
    "\n",
    "            if strategy == 'h2o' and current_cache_size > window_size:\n",
    "                # 1. Identify the \"heavy hitters\" (tokens with the highest attention scores)\n",
    "                _, heavy_hitter_indices = torch.topk(attention_scores[:current_cache_size], heavy_hitters_to_keep)\n",
    "                \n",
    "                # 2. Identify the most recent tokens\n",
    "                num_recent_to_keep = window_size - heavy_hitters_to_keep\n",
    "                recent_indices = torch.arange(current_cache_size - num_recent_to_keep, current_cache_size).to(device)\n",
    "                \n",
    "                # 3. Combine them and remove duplicates to get the final set of tokens to keep\n",
    "                indices_to_keep = torch.unique(torch.cat([heavy_hitter_indices, recent_indices]))\n",
    "                \n",
    "                # 4. Trim the cache to only keep these important indices\n",
    "                past_key_values = tuple(\n",
    "                    (\n",
    "                        layer_past[0].index_select(2, indices_to_keep),\n",
    "                        layer_past[1].index_select(2, indices_to_keep)\n",
    "                    ) for layer_past in past_key_values\n",
    "                )\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            timings.append((end_time - start_time) * 1000)\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Generated {i + 1}/{generation_length} tokens...\")\n",
    "    \n",
    "    print(f\"'{strategy}' test complete.\")\n",
    "    return timings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0167c6",
   "metadata": {},
   "source": [
    "## Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187ed851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run the Two Experiments ---\n",
    "window = 128\n",
    "heavy_hitters = 32 # The number of \"important\" past tokens to always protect\n",
    "\n",
    "baseline_timings = analyze_inference_h2o(strategy='baseline')\n",
    "h2o_timings = analyze_inference_h2o(strategy='h2o', window_size=window, heavy_hitters_to_keep=heavy_hitters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ba4fd6",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7079ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(baseline_timings, label='Baseline (Full KV Cache)')\n",
    "plt.plot(h2o_timings, label=f'Simplified H2O (Window={window}, Heavy Hitters={heavy_hitters})')\n",
    "\n",
    "plt.xlabel('Generated Token Number (Sequence Length)')\n",
    "plt.ylabel('Time per Token (ms)')\n",
    "plt.title(f'KV Cache Performance: Baseline vs. Simplified H2O on {model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hawai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
