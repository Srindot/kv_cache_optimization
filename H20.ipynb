{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61494020",
   "metadata": {},
   "source": [
    "# H2O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56d4d6f",
   "metadata": {},
   "source": [
    "## Headers Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3807ef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fcd3a5",
   "metadata": {},
   "source": [
    "## Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f6da72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model: gpt2-large (with attentions)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"gpt2-large\"\n",
    "generation_length = 512  # The number of tokens to generate for the test\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Loading model: {model_name} (with attentions)...\")\n",
    "\n",
    "# --- Load Model and Tokenizer ---\n",
    "# CRITICAL: We need 'output_attentions=True' to see which tokens are important.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True).to(device)\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0363cd82",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc660e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_inference_h2o(strategy, window_size=None, heavy_hitters_to_keep=4):\n",
    "    \"\"\"\n",
    "    Generates tokens and measures latency for baseline and a simplified H2O strategy.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running test for strategy: '{strategy}' ---\")\n",
    "    \n",
    "    input_ids = torch.tensor([[tokenizer.bos_token_id]], dtype=torch.long).to(device)\n",
    "    past_key_values = None\n",
    "    timings = []\n",
    "    \n",
    "    # We'll use this to track the \"importance\" of each token in the cache\n",
    "    attention_scores = torch.zeros(generation_length + 1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(generation_length):\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            # Model forward pass\n",
    "            outputs = model(input_ids=input_ids[:, -1:], past_key_values=past_key_values, use_cache=True)\n",
    "            \n",
    "            # --- Logic for H2O: Track Attention Scores ---\n",
    "            if strategy == 'h2o' and outputs.attentions is not None:\n",
    "                # Go through each attention layer\n",
    "                for layer_attention in outputs.attentions:\n",
    "                    # Sum the attention scores from the current token to all past tokens\n",
    "                    # This gives us a rough measure of how much the model \"looked back\" at each past token\n",
    "                    current_attention_to_past = layer_attention[0, :, -1, :-1].sum(dim=0)\n",
    "                    if current_attention_to_past.numel() > 0:\n",
    "                        attention_scores[:i] += current_attention_to_past\n",
    "\n",
    "            next_token_id = torch.argmax(outputs.logits[:, -1, :], dim=-1).unsqueeze(-1)\n",
    "            input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "            # --- KV Cache Eviction Logic ---\n",
    "            if past_key_values is None:\n",
    "                end_time = time.perf_counter()\n",
    "                timings.append((end_time - start_time) * 1000)\n",
    "                continue\n",
    "\n",
    "            current_cache_size = past_key_values[0][0].shape[2]\n",
    "\n",
    "            if strategy == 'h2o' and current_cache_size > window_size:\n",
    "                # 1. Identify the \"heavy hitters\" (tokens with the highest attention scores)\n",
    "                _, heavy_hitter_indices = torch.topk(attention_scores[:current_cache_size], heavy_hitters_to_keep)\n",
    "                \n",
    "                # 2. Identify the most recent tokens\n",
    "                num_recent_to_keep = window_size - heavy_hitters_to_keep\n",
    "                recent_indices = torch.arange(current_cache_size - num_recent_to_keep, current_cache_size).to(device)\n",
    "                \n",
    "                # 3. Combine them and remove duplicates to get the final set of tokens to keep\n",
    "                indices_to_keep = torch.unique(torch.cat([heavy_hitter_indices, recent_indices]))\n",
    "                \n",
    "                # 4. Trim the cache to only keep these important indices\n",
    "                past_key_values = tuple(\n",
    "                    (\n",
    "                        layer_past[0].index_select(2, indices_to_keep),\n",
    "                        layer_past[1].index_select(2, indices_to_keep)\n",
    "                    ) for layer_past in past_key_values\n",
    "                )\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            timings.append((end_time - start_time) * 1000)\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Generated {i + 1}/{generation_length} tokens...\")\n",
    "    \n",
    "    print(f\"'{strategy}' test complete.\")\n",
    "    return timings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0167c6",
   "metadata": {},
   "source": [
    "## Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "187ed851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running test for strategy: 'baseline' ---\n",
      "Generated 100/512 tokens...\n",
      "Generated 200/512 tokens...\n",
      "Generated 300/512 tokens...\n",
      "Generated 400/512 tokens...\n",
      "Generated 500/512 tokens...\n",
      "'baseline' test complete.\n",
      "\n",
      "--- Running test for strategy: 'h2o' ---\n",
      "Generated 100/512 tokens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.53.0. You should pass an instance of `Cache` instead, e.g. `past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (129) must match the size of tensor b (116) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m heavy_hitters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m \u001b[38;5;66;03m# The number of \"important\" past tokens to always protect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m baseline_timings \u001b[38;5;241m=\u001b[39m analyze_inference_h2o(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbaseline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m h2o_timings \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_inference_h2o\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh2o\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheavy_hitters_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheavy_hitters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m, in \u001b[0;36manalyze_inference_h2o\u001b[0;34m(strategy, window_size, heavy_hitters_to_keep)\u001b[0m\n\u001b[1;32m     27\u001b[0m         current_attention_to_past \u001b[38;5;241m=\u001b[39m layer_attention[\u001b[38;5;241m0\u001b[39m, :, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m current_attention_to_past\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 29\u001b[0m             attention_scores[:i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m current_attention_to_past\n\u001b[1;32m     31\u001b[0m next_token_id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([input_ids, next_token_id], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (129) must match the size of tensor b (116) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# --- Run the Two Experiments ---\n",
    "window = 128\n",
    "heavy_hitters = 32 # The number of \"important\" past tokens to always protect\n",
    "\n",
    "baseline_timings = analyze_inference_h2o(strategy='baseline')\n",
    "h2o_timings = analyze_inference_h2o(strategy='h2o', window_size=window, heavy_hitters_to_keep=heavy_hitters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ba4fd6",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7079ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(baseline_timings, label='Baseline (Full KV Cache)')\n",
    "plt.plot(h2o_timings, label=f'Simplified H2O (Window={window}, Heavy Hitters={heavy_hitters})')\n",
    "\n",
    "plt.xlabel('Generated Token Number (Sequence Length)')\n",
    "plt.ylabel('Time per Token (ms)')\n",
    "plt.title(f'KV Cache Performance: Baseline vs. Simplified H2O on {model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hawai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
