{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66175872",
   "metadata": {},
   "source": [
    "# System Level Quantiziation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "062dc18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running test for: Baseline (FP16 KV Cache) ---\n",
      "INFO 10-03 19:37:34 [utils.py:328] non-default args: {'disable_log_stats': True, 'enforce_eager': True, 'model': 'gpt2-large'}\n",
      "INFO 10-03 19:37:35 [__init__.py:742] Resolved architecture: GPT2LMHeadModel\n",
      "INFO 10-03 19:37:36 [__init__.py:2764] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 10-03 19:37:36 [__init__.py:1815] Using max model len 1024\n",
      "INFO 10-03 19:37:36 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 10-03 19:37:36 [cache.py:214] Possibly too large swap space. 4.00 GiB out of the 7.47 GiB total CPU memory is allocated for the swap space.\n",
      "INFO 10-03 19:37:36 [__init__.py:3400] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m INFO 10-03 19:37:38 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m INFO 10-03 19:37:38 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='gpt2-large', speculative_config=None, tokenizer='gpt2-large', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=gpt2-large, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m WARNING 10-03 19:37:38 [interface.py:391] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718] EngineCore failed to start.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718]   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 709, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718]   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 505, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718]   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 82, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718]     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718]   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718]     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718]   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 48, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718]     self.collective_rpc(\"init_device\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718]   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 58, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718]   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/utils/__init__.py\", line 3060, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718]   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 611, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718]     self.worker.init_device()  # type: ignore\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718]   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 180, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718]     raise ValueError(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ERROR 10-03 19:37:39 [core.py:718] ValueError: Free memory on device (3.2/4.0 GiB) on startup is less than desired GPU memory utilization (0.9, 3.6 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m Process EngineCore_DP0:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m     self.run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 722, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 709, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 505, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 82, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 48, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m     self.collective_rpc(\"init_device\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 58, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/utils/__init__.py\", line 3060, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 611, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m     self.worker.init_device()  # type: ignore\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 180, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m     raise ValueError(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m ValueError: Free memory on device (3.2/4.0 GiB) on startup is less than desired GPU memory utilization (0.9, 3.6 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m Exception ignored in: <function ExecutorBase.__del__ at 0x7d57a9d15cf0>\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 237, in __del__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m     self.shutdown()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 76, in shutdown\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m     worker.shutdown()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 528, in shutdown\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m     self.worker.shutdown()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m   File \"/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 675, in shutdown\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m     self.model_runner.ensure_kv_transfer_shutdown()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9439)\u001b[0;0m AttributeError: 'NoneType' object has no attribute 'ensure_kv_transfer_shutdown'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 61\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# --- Step 1: Run inference once to get baseline FP16 cache usage ---\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m fp16_vram_base, fp16_vram_cache, num_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference_and_measure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBaseline (FP16 KV Cache)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# --- Step 2: Simulate the INT8 cache usage ---\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# An INT8 cache (1 byte per value) is 50% the size of an FP16 cache (2 bytes per value).\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# This calculation simulates the memory usage if we had used INT8.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m int8_cache_simulated \u001b[38;5;241m=\u001b[39m fp16_vram_cache \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m\n",
      "Cell \u001b[0;32mIn[16], line 24\u001b[0m, in \u001b[0;36mrun_inference_and_measure\u001b[0;34m(model_path, label)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads a model, runs inference with a standard FP16 cache, and measures VRAM.\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Running test for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_cache_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use the default, which will be FP16 on your GPU\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.90\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m vram_before \u001b[38;5;241m=\u001b[39m get_vram_usage()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVRAM before generation (base model): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvram_before\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/entrypoints/llm.py:282\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m log_non_default_args(engine_args)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/engine/llm_engine.py:493\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[1;32m    491\u001b[0m     engine_cls \u001b[38;5;241m=\u001b[39m V1LLMEngine\n\u001b[0;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:134\u001b[0m, in \u001b[0;36mLLMEngine.from_vllm_config\u001b[0;34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_vllm_config\u001b[39m(\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    133\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLMEngine\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mExecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:111\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_processor\u001b[38;5;241m.\u001b[39mtracer \u001b[38;5;241m=\u001b[39m tracer\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_core \u001b[38;5;241m=\u001b[39m \u001b[43mEngineCoreClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_core\u001b[38;5;241m.\u001b[39mengine_core\u001b[38;5;241m.\u001b[39mmodel_executor  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:80\u001b[0m, in \u001b[0;36mEngineCoreClient.make_client\u001b[0;34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient\u001b[38;5;241m.\u001b[39mmake_async_mp_client(\n\u001b[1;32m     77\u001b[0m         vllm_config, executor_class, log_stats)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "File \u001b[0;32m~/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:602\u001b[0m, in \u001b[0;36mSyncMPClient.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[1;32m    601\u001b[0m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_dp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mdata_parallel_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_queue \u001b[38;5;241m=\u001b[39m queue\u001b[38;5;241m.\u001b[39mQueue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n",
      "File \u001b[0;32m~/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:448\u001b[0m, in \u001b[0;36mMPClient.__init__\u001b[0;34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats_update_address \u001b[38;5;241m=\u001b[39m client_addresses\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats_update_address\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;66;03m# Engines are managed by this client.\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m launch_core_engines(vllm_config, executor_class,\n\u001b[1;32m    449\u001b[0m                              log_stats) \u001b[38;5;28;01mas\u001b[39;00m (engine_manager,\n\u001b[1;32m    450\u001b[0m                                             coordinator,\n\u001b[1;32m    451\u001b[0m                                             addresses):\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\u001b[38;5;241m.\u001b[39mcoordinator \u001b[38;5;241m=\u001b[39m coordinator\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\u001b[38;5;241m.\u001b[39mengine_manager \u001b[38;5;241m=\u001b[39m engine_manager\n",
      "File \u001b[0;32m~/miniconda3/envs/hawai/lib/python3.10/contextlib.py:142\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/v1/engine/utils.py:729\u001b[0m, in \u001b[0;36mlaunch_core_engines\u001b[0;34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m local_engine_manager, coordinator, addresses\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# Now wait for engines to start.\u001b[39;00m\n\u001b[0;32m--> 729\u001b[0m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengines_to_handshake\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_engine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hawai/lib/python3.10/site-packages/vllm/v1/engine/utils.py:782\u001b[0m, in \u001b[0;36mwait_for_engine_startup\u001b[0;34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process\u001b[38;5;241m.\u001b[39mexitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m         finished[coord_process\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m coord_process\u001b[38;5;241m.\u001b[39mexitcode\n\u001b[0;32m--> 782\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine core initialization failed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    783\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee root cause above. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    784\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m eng_identity, ready_msg_bytes \u001b[38;5;241m=\u001b[39m handshake_socket\u001b[38;5;241m.\u001b[39mrecv_multipart()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from vllm import LLM, SamplingParams\n",
    "import gc\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MODEL_NAME = \"gpt2-large\"\n",
    "GENERATION_LENGTH = 1024\n",
    "TEMPERATURE = 0.8\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- VRAM MONITORING HELPER ---\n",
    "def get_vram_usage():\n",
    "    \"\"\"Returns the current GPU memory usage in GB.\"\"\"\n",
    "    if DEVICE == \"cuda\":\n",
    "        return torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    return 0\n",
    "\n",
    "def run_inference_and_measure(model_path, label):\n",
    "    \"\"\"Loads a model, runs inference with a standard FP16 cache, and measures VRAM.\"\"\"\n",
    "    print(f\"--- Running test for: {label} ---\")\n",
    "    \n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        kv_cache_dtype=\"auto\", # Use the default, which will be FP16 on your GPU\n",
    "        dtype=\"auto\",\n",
    "        gpu_memory_utilization=0.90,\n",
    "        enforce_eager=True \n",
    "    )\n",
    "\n",
    "    vram_before = get_vram_usage()\n",
    "    print(f\"VRAM before generation (base model): {vram_before:.3f} GB\")\n",
    "\n",
    "    prompt = \"The primary purpose of using Key-Value cache quantization in Large Language Models is to achieve\"\n",
    "    sampling_params = SamplingParams(temperature=TEMPERATURE, max_tokens=GENERATION_LENGTH)\n",
    "\n",
    "    outputs = llm.generate(prompt, sampling_params)\n",
    "    \n",
    "    vram_after = get_vram_usage()\n",
    "    print(f\"VRAM after generation (model + cache): {vram_after:.3f} GB\")\n",
    "\n",
    "    num_tokens = len(outputs[0].outputs[0].token_ids)\n",
    "    vram_for_cache = vram_after - vram_before\n",
    "    print(f\"Generated {num_tokens} tokens.\")\n",
    "    print(f\"VRAM used by FP16 KV Cache: {vram_for_cache:.3f} GB\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    del llm\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return vram_before, vram_for_cache, num_tokens\n",
    "\n",
    "def main():\n",
    "    if DEVICE == \"cpu\":\n",
    "        print(\"❌ This benchmark requires a CUDA-enabled GPU to measure VRAM.\")\n",
    "        return\n",
    "\n",
    "    # --- Step 1: Run inference once to get baseline FP16 cache usage ---\n",
    "    fp16_vram_base, fp16_vram_cache, num_tokens = run_inference_and_measure(\n",
    "        MODEL_NAME, \n",
    "        \"Baseline (FP16 KV Cache)\"\n",
    "    )\n",
    "\n",
    "    # --- Step 2: Simulate the INT8 cache usage ---\n",
    "    # An INT8 cache (1 byte per value) is 50% the size of an FP16 cache (2 bytes per value).\n",
    "    # This calculation simulates the memory usage if we had used INT8.\n",
    "    int8_cache_simulated = fp16_vram_cache / 2.0\n",
    "    memory_saved = fp16_vram_cache - int8_cache_simulated\n",
    "    \n",
    "    print(\"\\n--- Simulation Summary ---\")\n",
    "    print(f\"Sequence length: {num_tokens} tokens\")\n",
    "    print(f\"Measured FP16 KV Cache size:  {fp16_vram_cache:.3f} GB\")\n",
    "    print(f\"Simulated INT8 KV Cache size: {int8_cache_simulated:.3f} GB\")\n",
    "    print(f\"✅ VRAM saved on cache: {memory_saved:.3f} GB (a 50% reduction)\")\n",
    "\n",
    "    # --- Step 3: Plot the real vs. simulated results ---\n",
    "    labels = ['Baseline (FP16 Cache)', 'Simulated (INT8 Cache)']\n",
    "    cache_sizes = [fp16_vram_cache, int8_cache_simulated]\n",
    "    base_model_size = [fp16_vram_base, fp16_vram_base] # Base model size is the same\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.bar(labels, base_model_size, label='Base Model VRAM', color='#4c72b0')\n",
    "    plt.bar(labels, cache_sizes, bottom=base_model_size, label='KV Cache VRAM', color=['#c44e52', '#f5b041'])\n",
    "\n",
    "    # Add text labels for total VRAM\n",
    "    total_fp16 = fp16_vram_base + fp16_vram_cache\n",
    "    total_simulated = fp16_vram_base + int8_cache_simulated\n",
    "    plt.text(labels[0], total_fp16, f'Total: {total_fp16:.2f} GB', ha='center', va='bottom', fontsize=11)\n",
    "    plt.text(labels[1], total_simulated, f'Total: {total_simulated:.2f} GB', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "    plt.ylabel('Total VRAM Usage (GB)')\n",
    "    plt.title(f'VRAM Usage: Real FP16 vs. Simulated INT8 KV Cache ({num_tokens} tokens)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hawai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
