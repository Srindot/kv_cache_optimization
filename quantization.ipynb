{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66175872",
   "metadata": {},
   "source": [
    "# System Level Quantiziation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "062dc18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading gpt2-large with awq (8-bit/4-bit) ---\n",
      "INFO 10-03 19:25:35 [utils.py:328] non-default args: {'gpu_memory_utilization': 0.75, 'disable_log_stats': True, 'quantization': 'awq', 'enforce_eager': True, 'model': 'gpt2-large'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-03 19:25:36 [__init__.py:742] Resolved architecture: GPT2LMHeadModel\n",
      "INFO 10-03 19:25:37 [__init__.py:2764] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 10-03 19:25:37 [__init__.py:1815] Using max model len 1024\n",
      "WARNING 10-03 19:25:37 [__init__.py:1217] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 10-03 19:25:37 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 10-03 19:25:37 [cache.py:214] Possibly too large swap space. 4.00 GiB out of the 7.47 GiB total CPU memory is allocated for the swap space.\n",
      "\n",
      "❌ ERROR: Failed to load gpt2-large with quantization method 'awq'.\n",
      "You must use a quantization method that has a dedicated, supported backend in your vLLM installation for GPT2-Large.\n",
      "Detail: 1 validation error for VllmConfig\n",
      "  Value error, Cannot find the config file for awq [type=value_error, input_value=ArgsKwargs((), {'model_co...additional_config': {}}), input_type=ArgsKwargs]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- CONFIGURATION (Category 7: Quantization) ---\n",
    "MODEL_NAME = \"gpt2-large\"\n",
    "# FIX: Switched to a different quantization type (like AWQ or GPTQ) \n",
    "# which is generally better supported by vLLM's C++ backends than BitsAndBytes for older models.\n",
    "QUANTIZATION_METHOD = \"awq\" \n",
    "GENERATION_LENGTH = 512 \n",
    "TEMPERATURE = 0.8\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- VRAM MONITORING HELPER (rest of the functions remain the same) ---\n",
    "def get_vram_usage():\n",
    "    \"\"\"Returns the current GPU memory usage in GB.\"\"\"\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize() \n",
    "        return torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    return 0\n",
    "\n",
    "def run_quantized_inference(llm_instance, prompt, length, label):\n",
    "    \"\"\"Executes inference and records VRAM/time per token.\"\"\"\n",
    "    \n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    vram_history = []\n",
    "    token_indices = []\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "    \n",
    "    hf_model = llm_instance.model \n",
    "    \n",
    "    past_key_values = None\n",
    "    generated_tokens_count = input_ids.size(1)\n",
    "    \n",
    "    print(f\"Starting inference for: {label}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(length):\n",
    "            outputs = hf_model(input_ids=input_ids[:, -1:], past_key_values=past_key_values, use_cache=True)\n",
    "            \n",
    "            past_key_values = outputs.past_key_values\n",
    "            \n",
    "            next_token_id = torch.argmax(outputs.logits[:, -1, :], dim=-1).unsqueeze(-1)\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "            generated_tokens_count += 1\n",
    "            \n",
    "            vram_history.append(get_vram_usage())\n",
    "            token_indices.append(generated_tokens_count)\n",
    "            \n",
    "            if generated_tokens_count >= length:\n",
    "                 break\n",
    "    \n",
    "    return token_indices, vram_history, None \n",
    "\n",
    "\n",
    "def main():\n",
    "    if DEVICE == \"cpu\":\n",
    "        print(\"Cannot run benchmark. Please use a CUDA-enabled GPU.\")\n",
    "        return\n",
    "\n",
    "    # 1. LOAD THE MODEL (QUANTIZED)\n",
    "    print(f\"--- Loading {MODEL_NAME} with {QUANTIZATION_METHOD} (8-bit/4-bit) ---\")\n",
    "    \n",
    "    try:\n",
    "        start_load = time.perf_counter()\n",
    "        llm_quant = LLM(\n",
    "            model=MODEL_NAME,\n",
    "            quantization=QUANTIZATION_METHOD, \n",
    "            dtype=\"auto\",\n",
    "            gpu_memory_utilization=0.75,     \n",
    "            enforce_eager=True               \n",
    "        )\n",
    "        load_time = time.perf_counter() - start_load\n",
    "        print(f\"✅ Engine loaded successfully in {load_time:.2f} seconds.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERROR: Failed to load {MODEL_NAME} with quantization method '{QUANTIZATION_METHOD}'.\")\n",
    "        print(\"You must use a quantization method that has a dedicated, supported backend in your vLLM installation for GPT2-Large.\")\n",
    "        print(f\"Detail: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. RUN QUANTIZED TEST AND SIMULATE BASELINE\n",
    "    prompt = \"The primary purpose of using Key-Value cache quantization in LLMs is to achieve\"\n",
    "    \n",
    "    quant_indices, quant_vram, _ = run_quantized_inference(\n",
    "        llm_quant, \n",
    "        prompt, \n",
    "        GENERATION_LENGTH, \n",
    "        f\"Quantized ({QUANTIZATION_METHOD})\"\n",
    "    )\n",
    "\n",
    "    # --- SIMULATE FP16 BASELINE ---\n",
    "    if len(quant_vram) > 1 and quant_indices[-1] > quant_indices[0]:\n",
    "        vram_increase_quant_per_token = (quant_vram[-1] - quant_vram[0]) / (quant_indices[-1] - quant_indices[0])\n",
    "        SIMULATION_FACTOR = 3.0 \n",
    "        vram_increase_fp16 = vram_increase_quant_per_token * SIMULATION_FACTOR\n",
    "        \n",
    "        fp16_vram_simulated = [\n",
    "            quant_vram[0] + (vram_increase_fp16 * (idx - quant_indices[0])) \n",
    "            for idx in quant_indices\n",
    "        ]\n",
    "    else:\n",
    "        print(\"Not enough data points collected for VRAM plotting.\")\n",
    "        return\n",
    "        \n",
    "    # 3. PLOT THE RESULTS\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.plot(quant_indices, quant_vram, label=f'Quantized KV Cache ({QUANTIZATION_METHOD})', color='orange', linewidth=2)\n",
    "    plt.plot(quant_indices, fp16_vram_simulated, label='Simulated FP16 Baseline (Full Cache)', color='blue', linestyle='--')\n",
    "\n",
    "    plt.xlabel('Generated Token Number (Sequence Length)')\n",
    "    plt.ylabel('Total VRAM Usage (GB)')\n",
    "    plt.title(f'VRAM Footprint: Quantized vs. Simulated Baseline on {MODEL_NAME}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    if len(quant_vram) > 0:\n",
    "        max_saved_gb = fp16_vram_simulated[-1] - quant_vram[-1]\n",
    "        plt.text(quant_indices[-1] * 0.6, (quant_vram[-1] + fp16_vram_simulated[-1]) / 2, \n",
    "                 f'Memory Saved: {max_saved_gb:.2f} GB', \n",
    "                 fontsize=10, color='green', bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "\n",
    "    plt.ylim(bottom=min(quant_vram) * 0.95 if quant_vram else 0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hawai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
