# KV Cache Management for LLMs
This repository contains the course project of the course Hardware for AI, the course project aims to optimize the kv caching for a particular arch accelerator design.

# Problem Statement
Explore strategies to reduce redundant computations and improve memory utilization during long-context real-time LLM inferencing
Ref: A Survey on Large Language Model
Acceleration based on KV Cache Management

