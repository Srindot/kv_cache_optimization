{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "561605f5",
   "metadata": {},
   "source": [
    "# Sliding Cache window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a044b",
   "metadata": {},
   "source": [
    "## Modules to Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cc417ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sriney/miniconda3/envs/hawai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Model Pulling\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "#\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46bb238",
   "metadata": {},
   "source": [
    "## Model Pulling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c133b196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Downloading model: gpt2 ---\n",
      "This may take a few moments depending on your internet connection.\n",
      "\n",
      "Model and tokenizer are downloaded and cached successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the model you want to download.\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "print(f\"--- Downloading model: {model_name} ---\")\n",
    "print(\"This may take a few moments depending on your internet connection.\")\n",
    "\n",
    "# By calling 'from_pretrained', the library checks the local cache.\n",
    "# If the model isn't there, it downloads and saves it.\n",
    "# If it is there, it does nothing.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "print(\"\\nModel and tokenizer are downloaded and cached successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ceb507",
   "metadata": {},
   "source": [
    "## Model Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace94b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 'gpt2' from local cache...\n",
      "Running inference...\n",
      "\n",
      "--- Generated Text ---\n",
      "The future of artificial intelligence is uncertain.\n",
      "\n",
      "\"We're not sure what the future will look like,\" said Dr. Michael S. Schoenfeld, a professor of computer science at the University of California, Berkeley. \"But we're not\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "# Define the model to use (should match the one you downloaded).\n",
    "model_name = \"gpt2\"\n",
    "# Define the input text for the model.\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "# --- Load and Run ---\n",
    "print(f\"Loading model '{model_name}' from local cache...\")\n",
    "# This will be fast because the model is already downloaded.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Running inference...\")\n",
    "# Encode the prompt and generate the output text.\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=50,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# --- Print the Result ---\n",
    "print(\"\\n--- Generated Text ---\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6044421b",
   "metadata": {},
   "source": [
    "# Cuda Check and model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9e6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model: gpt2-medium\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"gpt2-medium\"\n",
    "generation_length = 512  # How many tokens to generate for the test\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "# --- Load Model and Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86e0caa",
   "metadata": {},
   "source": [
    "## Analysis Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9486e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_inference(strategy, window_size=None):\n",
    "    \"\"\"\n",
    "    Generates tokens one-by-one and records the time taken for each.\n",
    "\n",
    "    Args:\n",
    "        strategy (str): 'baseline' or 'sliding_window'.\n",
    "        window_size (int, optional): The window size for the sliding window strategy.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running test for strategy: '{strategy}' ---\")\n",
    "    \n",
    "    # Start with a single \"beginning of sentence\" token\n",
    "    input_ids = torch.tensor([[tokenizer.bos_token_id]], dtype=torch.long).to(device)\n",
    "    past_key_values = None\n",
    "    timings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(generation_length):\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            # Model forward pass\n",
    "            outputs = model(input_ids=input_ids[:, -1:], past_key_values=past_key_values, use_cache=True)\n",
    "            \n",
    "            # Get the next predicted token\n",
    "            next_token_id = torch.argmax(outputs.logits[:, -1, :], dim=-1).unsqueeze(-1)\n",
    "            \n",
    "            # Append the new token for the next iteration\n",
    "            input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "            \n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "            # --- This is the Sliding Window Implementation ---\n",
    "            if strategy == 'sliding_window' and past_key_values is not None:\n",
    "                # After each step, trim the cache to the window size\n",
    "                trimmed_past = []\n",
    "                for layer_past in past_key_values:\n",
    "                    # Trim the sequence length dimension (dim 2)\n",
    "                    trimmed_key = layer_past[0][:, :, -window_size:, :]\n",
    "                    trimmed_value = layer_past[1][:, :, -window_size:, :]\n",
    "                    trimmed_past.append((trimmed_key, trimmed_value))\n",
    "                past_key_values = tuple(trimmed_past)\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            timings.append((end_time - start_time) * 1000)  # Store time in milliseconds\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Generated {i + 1}/{generation_length} tokens...\")\n",
    "    \n",
    "    print(f\"'{strategy}' test complete.\")\n",
    "    return timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47936cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experiment 1: Baseline (Without Optimization) ---\n",
    "# The cache will grow up to the full generation length.\n",
    "baseline_timings = analyze_inference(strategy='baseline')\n",
    "\n",
    "# --- Experiment 2: Sliding Window (With Optimization) ---\n",
    "# We cap the cache size at 128 tokens.\n",
    "sliding_window_timings = analyze_inference(strategy='sliding_window', window_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hawai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
