{
  "optimizer_name": "Grouped Query Attention",
  "config": {
    "model_name": "gpt2-large",
    "generation_length": 50,
    "device": "cuda",
    "batch_size": 1
  },
  "timings": [
    147.29699800045637,
    150.82059700034733
  ],
  "vram_usage": [
    1017.5,
    1017.5
  ],
  "metadata": {
    "total_experiment_time": 0.29837822914123535,
    "optimizer_config": {
      "name": "GQA-4groups",
      "device": "cuda",
      "results": {},
      "num_groups": 4
    }
  },
  "average_timing": 149.05879750040185,
  "peak_vram": 1017.5
}