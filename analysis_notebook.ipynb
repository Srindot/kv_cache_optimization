{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae219415",
   "metadata": {},
   "source": [
    "# KV Cache Optimization Results Analysis and Visualization\n",
    "\n",
    "This notebook provides comprehensive analysis and visualization of KV cache optimization experiment results.\n",
    "\n",
    "## Features:\n",
    "- Load and analyze saved experiment results\n",
    "- Generate comprehensive comparison plots\n",
    "- Interactive analysis of different optimization strategies\n",
    "- Performance metrics and summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa5daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('./src')\n",
    "\n",
    "from src.runner import ExperimentRunner, ExperimentConfig\n",
    "from src.plotting import ResultsPlotter, create_summary_report\n",
    "from src.base import ExperimentResults\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e91d4a3",
   "metadata": {},
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RESULTS_DIR = \"./results\"\n",
    "PLOTS_DIR = \"./results/plots\"\n",
    "LOAD_FROM_SAVED = True  # Set to False to run new experiments\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize plotter\n",
    "plotter = ResultsPlotter(figsize=(14, 8))\n",
    "\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Plots directory: {PLOTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5332c981",
   "metadata": {},
   "source": [
    "## Load or Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baced84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_results(results_dir: str) -> dict:\n",
    "    \"\"\"Load the most recent results for each experiment type.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Find all pickle files\n",
    "    pickle_files = glob.glob(os.path.join(results_dir, \"*.pkl\"))\n",
    "    \n",
    "    if not pickle_files:\n",
    "        print(\"No saved results found. Please run experiments first.\")\n",
    "        return {}\n",
    "    \n",
    "    # Group by experiment name and get latest\n",
    "    experiment_files = {}\n",
    "    for file in pickle_files:\n",
    "        filename = os.path.basename(file)\n",
    "        # Extract experiment name (everything before the timestamp)\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            exp_name = '_'.join(parts[:-2])  # Everything except timestamp and .pkl\n",
    "            if exp_name not in experiment_files or file > experiment_files[exp_name]:\n",
    "                experiment_files[exp_name] = file\n",
    "    \n",
    "    # Load the results\n",
    "    config = ExperimentConfig()\n",
    "    runner = ExperimentRunner(config)\n",
    "    \n",
    "    for exp_name, file_path in experiment_files.items():\n",
    "        try:\n",
    "            result = runner.load_results(file_path)\n",
    "            results[exp_name] = result\n",
    "            print(f\"Loaded: {exp_name} from {os.path.basename(file_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {exp_name}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if LOAD_FROM_SAVED:\n",
    "    results = load_latest_results(RESULTS_DIR)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No saved results found. Running new experiments...\")\n",
    "        LOAD_FROM_SAVED = False\n",
    "\n",
    "if not LOAD_FROM_SAVED:\n",
    "    print(\"Running new experiments...\")\n",
    "    from src.runner import run_full_comparison\n",
    "    \n",
    "    results = run_full_comparison(\n",
    "        model_name=\"gpt2-large\",\n",
    "        generation_length=512,\n",
    "        save_results=True,\n",
    "        results_dir=RESULTS_DIR\n",
    "    )\n",
    "\n",
    "print(f\"\\nLoaded {len(results)} experiment results:\")\n",
    "for name in results.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9413566",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b81491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "if results:\n",
    "    summary_data = []\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        summary_data.append({\n",
    "            'Experiment': name,\n",
    "            'Avg Timing (ms/token)': f\"{result.get_average_timing():.2f}\",\n",
    "            'Peak VRAM (GB)': f\"{result.get_peak_vram():.2f}\",\n",
    "            'Total Tokens': len(result.timings),\n",
    "            'Model': result.config.model_name,\n",
    "            'Generation Length': result.config.generation_length\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(\"EXPERIMENT SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Calculate improvements relative to baseline\n",
    "    baseline_result = None\n",
    "    for name, result in results.items():\n",
    "        if 'baseline' in name.lower():\n",
    "            baseline_result = result\n",
    "            break\n",
    "    \n",
    "    if baseline_result:\n",
    "        print(\"\\n\\nPERFORMANCE IMPROVEMENTS (vs Baseline)\")\n",
    "        print(\"=\" * 50)\n",
    "        baseline_time = baseline_result.get_average_timing()\n",
    "        baseline_vram = baseline_result.get_peak_vram()\n",
    "        \n",
    "        for name, result in results.items():\n",
    "            if 'baseline' not in name.lower():\n",
    "                time_improvement = ((baseline_time - result.get_average_timing()) / baseline_time) * 100\n",
    "                vram_improvement = ((baseline_vram - result.get_peak_vram()) / baseline_vram) * 100\n",
    "                \n",
    "                print(f\"{name}:\")\n",
    "                print(f\"  Timing improvement: {time_improvement:+.1f}%\")\n",
    "                print(f\"  VRAM improvement: {vram_improvement:+.1f}%\")\n",
    "                print()\n",
    "else:\n",
    "    print(\"No results to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51225da7",
   "metadata": {},
   "source": [
    "## Timing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Plot timing comparison\n",
    "    plotter.plot_timing_comparison(results, show_plot=True)\n",
    "else:\n",
    "    print(\"No results to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfabc8f",
   "metadata": {},
   "source": [
    "## VRAM Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069fa3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Plot VRAM comparison\n",
    "    plotter.plot_vram_comparison(results, show_plot=True)\n",
    "else:\n",
    "    print(\"No results to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7727b6",
   "metadata": {},
   "source": [
    "## Combined Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50676c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Plot combined analysis\n",
    "    plotter.plot_combined_analysis(results, show_plot=True)\n",
    "else:\n",
    "    print(\"No results to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0749e60",
   "metadata": {},
   "source": [
    "## Performance Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Plot performance bars\n",
    "    plotter.plot_performance_bars(results, show_plot=True)\n",
    "else:\n",
    "    print(\"No results to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97154825",
   "metadata": {},
   "source": [
    "## Throughput Comparison (Memory Optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c688d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Plot throughput comparison for memory optimizers\n",
    "    plotter.plot_throughput_comparison(results, show_plot=True)\n",
    "else:\n",
    "    print(\"No results to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2cfdc1",
   "metadata": {},
   "source": [
    "## Interactive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016657d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive analysis section\n",
    "if results:\n",
    "    print(\"Available experiments for detailed analysis:\")\n",
    "    for i, name in enumerate(results.keys()):\n",
    "        print(f\"{i+1}. {name}\")\n",
    "    \n",
    "    # You can select specific experiments for detailed analysis\n",
    "    selected_experiments = list(results.keys())  # Analyze all by default\n",
    "    \n",
    "    print(f\"\\nAnalyzing {len(selected_experiments)} experiments...\")\n",
    "    \n",
    "    # Create detailed statistics\n",
    "    for name in selected_experiments:\n",
    "        result = results[name]\n",
    "        print(f\"\\n{name} - Detailed Statistics:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if result.timings:\n",
    "            timings_array = np.array(result.timings)\n",
    "            print(f\"Timing Statistics (ms/token):\")\n",
    "            print(f\"  Mean: {np.mean(timings_array):.2f}\")\n",
    "            print(f\"  Median: {np.median(timings_array):.2f}\")\n",
    "            print(f\"  Std Dev: {np.std(timings_array):.2f}\")\n",
    "            print(f\"  Min: {np.min(timings_array):.2f}\")\n",
    "            print(f\"  Max: {np.max(timings_array):.2f}\")\n",
    "        \n",
    "        if result.vram_usage:\n",
    "            vram_array = np.array(result.vram_usage)\n",
    "            print(f\"VRAM Statistics (GB):\")\n",
    "            print(f\"  Peak: {np.max(vram_array):.2f}\")\n",
    "            print(f\"  Mean: {np.mean(vram_array):.2f}\")\n",
    "            print(f\"  Initial: {vram_array[0]:.2f}\")\n",
    "            print(f\"  Final: {vram_array[-1]:.2f}\")\n",
    "            print(f\"  Growth: {vram_array[-1] - vram_array[0]:.2f}\")\n",
    "else:\n",
    "    print(\"No results available for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ea6d75",
   "metadata": {},
   "source": [
    "## Save All Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6c8e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    print(\"Saving all plots to disk...\")\n",
    "    \n",
    "    # Create all plots and save them\n",
    "    plotter.create_all_plots(results, save_dir=PLOTS_DIR, show_plots=False)\n",
    "    \n",
    "    # Create and save summary report\n",
    "    report_path = os.path.join(RESULTS_DIR, \"summary_report.txt\")\n",
    "    report = create_summary_report(results, report_path)\n",
    "    \n",
    "    print(f\"\\nAll plots saved to: {PLOTS_DIR}\")\n",
    "    print(f\"Summary report saved to: {report_path}\")\n",
    "    \n",
    "    # List all generated files\n",
    "    plot_files = glob.glob(os.path.join(PLOTS_DIR, \"*.png\"))\n",
    "    print(f\"\\nGenerated {len(plot_files)} plot files:\")\n",
    "    for file in sorted(plot_files):\n",
    "        print(f\"  - {os.path.basename(file)}\")\n",
    "else:\n",
    "    print(\"No results available to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fad13e",
   "metadata": {},
   "source": [
    "## Custom Analysis Section\n",
    "\n",
    "Use this section for custom analysis and experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3db5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom analysis - modify as needed\n",
    "if results:\n",
    "    # Example: Compare efficiency (inverse of timing)\n",
    "    efficiency_data = []\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        avg_timing = result.get_average_timing()\n",
    "        efficiency = 1000 / avg_timing if avg_timing > 0 else 0  # tokens per second\n",
    "        efficiency_data.append({'Experiment': name, 'Efficiency (tokens/sec)': efficiency})\n",
    "    \n",
    "    efficiency_df = pd.DataFrame(efficiency_data)\n",
    "    efficiency_df = efficiency_df.sort_values('Efficiency (tokens/sec)', ascending=False)\n",
    "    \n",
    "    print(\"EFFICIENCY RANKING (tokens/second):\")\n",
    "    print(\"=\" * 40)\n",
    "    for i, row in efficiency_df.iterrows():\n",
    "        print(f\"{row['Experiment']}: {row['Efficiency (tokens/sec)']:.2f}\")\n",
    "    \n",
    "    # Plot efficiency comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(efficiency_df['Experiment'], efficiency_df['Efficiency (tokens/sec)'])\n",
    "    plt.title('Efficiency Comparison (Higher is Better)')\n",
    "    plt.ylabel('Tokens per Second')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results available for custom analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cc48b1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides comprehensive analysis of KV cache optimization techniques. Key insights:\n",
    "\n",
    "1. **Baseline Performance**: Establishes the reference point for all comparisons\n",
    "2. **Attention Sink**: Reduces memory usage while maintaining reasonable performance\n",
    "3. **MiniCache**: Optimizes cache sharing across transformer layers\n",
    "4. **Memory Optimizers**: Compare different inference engines (vLLM vs Transformers)\n",
    "\n",
    "The results demonstrate the trade-offs between memory efficiency and computational performance across different optimization strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hawai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
